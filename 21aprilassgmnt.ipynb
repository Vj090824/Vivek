{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60e5f56-4963-4aa4-81e5-fe26c0278abf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3351001986.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    - Mathematically, the Euclidean distance between two points (x1, y1) and (x2, y2) in a 2D space is given by:\u001b[0m\n\u001b[0m                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    The main difference between the Euclidean distance metric and the Manhattan distance metric\n",
    "    in K-nearest neighbors (KNN) is how they measure the distance between data points.\n",
    "\n",
    "1. Euclidean Distance:\n",
    "   - Also known as L2 distance.\n",
    "   - It calculates the straight-line or \"as-the-crow-flies\" distance between two points \n",
    "    in a multidimensional space.\n",
    "   - Mathematically, the Euclidean distance between two points (x1, y1) and (x2, y2) in a 2D space is given by: \n",
    "     Euclidean Distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "   - In higher dimensions, it generalizes to:\n",
    "     Euclidean Distance = sqrt((x2 - x1)^2 + (y2 - y1)^2 + ... + (xn - x1)^2)\n",
    "\n",
    "2. Manhattan Distance:\n",
    "   - Also known as L1 distance or city-block distance.\n",
    "   - It measures the distance between two points as the sum of the absolute differences\n",
    "    of their coordinates along each dimension.\n",
    "   - Mathematically, the Manhattan distance between two points (x1, y1) and (x2, y2) in a 2D space is given by: \n",
    "     Manhattan Distance = |x2 - x1| + |y2 - y1|\n",
    "   - In higher dimensions, it generalizes to:\n",
    "     Manhattan Distance = |x2 - x1| + |y2 - y1| + ... + |xn - x1|\n",
    "\n",
    "The choice between Euclidean distance and Manhattan distance can significantly\n",
    "affect the performance of a KNN classifier or regressor, depending on the nature \n",
    "of the data and the problem you are trying to solve:\n",
    "\n",
    "1. Euclidean Distance:\n",
    "   - Works well when the underlying data distribution is isotropic (uniform in all directions)\n",
    "and the features are scaled similarly.\n",
    "   - Sensitive to outliers because it considers the squared differences between coordinates.\n",
    "   - It tends to emphasize the significance of larger differences in any single dimension,\n",
    "which may not be appropriate for some datasets.\n",
    "\n",
    "2. Manhattan Distance:\n",
    "   - Robust to outliers since it considers the absolute differences between coordinates.\n",
    "   - Works well when the features have different units or when the data distribution is not isotropic.\n",
    "   - It may perform better than Euclidean distance when dealing with data in which different dimensions\n",
    "are on different scales or when the relationships between features are not linear.\n",
    "\n",
    "In summary, the choice of distance metric should be based on the characteristics of your dataset \n",
    "and the problem you are solving. Experimenting with both Euclidean and Manhattan distances is \n",
    "often a good practice to determine which one works better for your specific application. \n",
    "Additionally, other distance metrics, such as Minkowski distance, can be used to strike \n",
    "a balance between these two distance metrics by introducing a parameter\n",
    "(p) that allows you to adjust the emphasis on different dimensions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    Choosing the optimal value of k for a K-Nearest Neighbors (KNN) classifier or regressor is\n",
    "    a crucial step in achieving good model performance. The choice of k significantly impacts\n",
    "    the model's ability to generalize from the data. Here are some techniques that \n",
    "    can be used to determine the optimal k value:\n",
    "\n",
    "1. **Brute Force Search:**\n",
    "   - Start with a small value of k, like 1, and incrementally increase it.\n",
    "   - Train the KNN model with each k value.\n",
    "   - Use cross-validation to evaluate the model's performance for each k.\n",
    "   - Select the k that results in the best performance (e.g., highest accuracy or lowest error).\n",
    "\n",
    "2. **Odd vs. Even k:**\n",
    "   - If you are working with binary classification problems, it's a good practice to choose an\n",
    "    odd value for k. This helps avoid ties when determining the class of a new data point.\n",
    "\n",
    "3. **Elbow Method:**\n",
    "   - For classification problems, you can use the elbow method to choose the optimal k.\n",
    "   - Plot the accuracy (or another performance metric) against different values of k.\n",
    "   - Look for the \"elbow point\" on the curve, where the accuracy starts to plateau. \n",
    "    This is often a good choice for k.\n",
    "\n",
    "4. **Cross-Validation:**\n",
    "   - Use techniques like k-fold cross-validation to estimate the model's performance\n",
    "    for different k values.\n",
    "   - Calculate the average performance metric (e.g., accuracy or mean squared error)\n",
    "for each k.\n",
    "   - Select the k that gives the best average performance.\n",
    "\n",
    "5. **Grid Search:**\n",
    "   - Combine hyperparameter tuning techniques like Grid Search or Random Search with cross-validation.\n",
    "   - Define a range of k values to explore.\n",
    "   - Use grid search to systematically evaluate the model's performance for\n",
    "    each k and other hyperparameters.\n",
    "\n",
    "6. **Distance Metrics:**\n",
    "   - Experiment with different distance metrics (e.g., Euclidean, Manhattan, or Minkowski)\n",
    "    in combination with various k values.\n",
    "   - Different distance metrics can impact the performance of KNN, so try to find\n",
    "the combination that works best for your data.\n",
    "\n",
    "7. **Domain Knowledge:**\n",
    "   - Consider domain-specific knowledge when choosing k.\n",
    "   - If you know that the underlying data distribution has certain characteristics,\n",
    "it can help guide your choice of k.\n",
    "\n",
    "8. **Use a Validation Set:**\n",
    "   - Split your dataset into training, validation, and test sets.\n",
    "   - Train KNN models with different k values on the training set.\n",
    "   - Use the validation set to select the k that performs best in terms of your chosen metric.\n",
    "\n",
    "9. **Bias-Variance Trade-off:**\n",
    "   - Keep in mind the bias-variance trade-off when choosing k. Smaller values of k lead to\n",
    "    more flexible models (lower bias but higher variance), while larger values of k result\n",
    "    in smoother models (higher bias but lower variance).\n",
    "\n",
    "10. **Experiment and Iterate:**\n",
    "    - Don't hesitate to experiment with different k values and techniques.\n",
    "    - Iterate through the process to refine your choice of k if needed.\n",
    "\n",
    "Remember that the optimal k value may vary from one dataset to another, so it's essential to\n",
    "perform this selection process for each specific problem you are working on. Additionally,\n",
    "consider the computational resources available, as larger values of \n",
    "k can be more computationally expensive.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "The choice of distance metric in a K-Nearest Neighbors (KNN) classifier or regressor can significantly\n",
    "affect its performance. The distance metric determines how the algorithm measures the similarity or \n",
    "dissimilarity between data points, and this, in turn, impacts the accuracy and robustness of the model.\n",
    "There are several common distance metrics used in KNN, including Euclidean distance, Manhattan distance,\n",
    "Minkowski distance, and others. Here's how the choice of distance metric can affect KNN performance\n",
    "    and when to choose one over the other:\n",
    "\n",
    "1. **Euclidean Distance**:\n",
    "   - **Use Case**: Euclidean distance is the most common choice and is suitable when features are continuous \n",
    "and have similar scales.\n",
    "   - **Effect on Performance**: It works well when data is distributed uniformly and isotropically\n",
    "    \n",
    "    (i.e., in all directions equally). However, it is sensitive to outliers and can be affected by\n",
    "    features with different variances.\n",
    "\n",
    "2. **Manhattan Distance (L1 Norm)**:\n",
    "   - **Use Case**: Manhattan distance is suitable when dealing with data that may have different scales\n",
    "or when you want the model to be less sensitive to outliers.\n",
    "   - **Effect on Performance**: It's less sensitive to outliers compared to Euclidean distance and can\n",
    "    work better when features have different units or scales. However, it may not perform well when the\n",
    "    data distribution is not approximately isotropic.\n",
    "\n",
    "3. **Minkowski Distance**:\n",
    "   - **Use Case**: Minkowski distance is a generalization of both Euclidean and Manhattan distances.\n",
    "You can adjust the parameter 'p' to switch between the two. \n",
    "This metric is flexible and can handle different scenarios.\n",
    "   - **Effect on Performance**: The choice of 'p' in Minkowski distance allows\n",
    "    you to adapt to the data's characteristics. A lower 'p' value (closer to 1) emphasizes\n",
    "    the Manhattan distance-like behavior, while a higher 'p' value (closer to 2) emphasizes\n",
    "    the Euclidean distance-like behavior.\n",
    "\n",
    "4. **Chebyshev Distance (Lâˆž Norm)**:\n",
    "   - **Use Case**: Chebyshev distance measures similarity based on the maximum absolute \n",
    "difference between corresponding coordinates. It's suitable when you want to \n",
    "focus on the most significant differences.\n",
    "   - **Effect on Performance**: It's very robust against outliers but may not work well \n",
    "    when you need to consider the magnitudes of differences.\n",
    "\n",
    "5. **Cosine Similarity**:\n",
    "   - **Use Case**: Cosine similarity measures the cosine of the angle between two vectors,\n",
    "making it appropriate for text or high-dimensional data, where the magnitude of vectors may not matter.\n",
    "   - **Effect on Performance**: It's useful when you want to capture the direction\n",
    "    or orientation of data points rather than their magnitudes.\n",
    "\n",
    "6. **Hamming Distance**:\n",
    "   - **Use Case**: Hamming distance is used for categorical data, where\n",
    "data points have binary attributes (0 or 1).\n",
    "   - **Effect on Performance**: It's suitable for handling categorical data but\n",
    "    doesn't work well for continuous data.\n",
    "\n",
    "7. **Custom Distance Metrics**:\n",
    "   - **Use Case**: In some cases, domain-specific knowledge may suggest a custom \n",
    "distance metric that better reflects the problem's characteristics.\n",
    "   - **Effect on Performance**: Custom metrics can be tailored to address specific challenges in your data.\n",
    "\n",
    "In summary, the choice of distance metric should be based on the nature of your data\n",
    "and the problem at hand. It's often a good practice to experiment with multiple distance\n",
    "metrics and cross-validation to determine which one works best for your specific dataset.\n",
    "Additionally, feature scaling and preprocessing can also impact the performance \n",
    "of KNN with different distance metrics, so it's essential to consider these factors as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    \n",
    "    K-Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that can be used for \n",
    "    classification and regression tasks. Like many machine learning algorithms, KNN has hyperparameters \n",
    "    that can be tuned to optimize its performance. Some common hyperparameters in KNN classifiers \n",
    "    and regressors include:\n",
    "\n",
    "1. **K (Number of Neighbors)**: K determines how many nearest neighbors to consider when making predictions.\n",
    "Smaller values of K may lead to more complex, potentially overfit models, while larger values may\n",
    "result in overly smoothed predictions. It's essential to choose an appropriate K value based on \n",
    "the problem and dataset characteristics. You can perform hyperparameter tuning by trying \n",
    "different K values and selecting the one that results in the best performance, often \n",
    "using techniques like cross-validation.\n",
    "\n",
    "2. **Distance Metric**: KNN relies on a distance metric (e.g., Euclidean distance,\n",
    "Manhattan distance, etc.) to measure the similarity between data points. The choice of \n",
    "distance metric can significantly impact the model's performance. Experiment with different\n",
    "distance metrics to see which one works best for your data. Some metrics may be more suitable\n",
    "for certain types of data or domains.\n",
    "\n",
    "3. **Weighting Scheme**: KNN allows you to assign different weights to neighbors when\n",
    "making predictions. Two common weighting schemes are uniform (all neighbors are given equal weight)\n",
    "and distance-based (closer neighbors have more influence). The choice of weighting can influence \n",
    "how the model responds to different data patterns. You can try both weighting schemes \n",
    "and see which one performs better.\n",
    "\n",
    "4. **Algorithm**: There are variations of the KNN algorithm, such as Ball Tree or KD Tree, \n",
    "which are used to accelerate the nearest neighbor search. The choice of algorithm can affect\n",
    "the model's training and prediction time. It's worth trying different algorithms, especially \n",
    "for large datasets, to see which one works efficiently for your data.\n",
    "\n",
    "5. **Feature Scaling**: KNN is sensitive to the scale of features since it relies on \n",
    "distance calculations. It's often necessary to scale or normalize your features to ensure\n",
    "that no single feature dominates the distance computation. Common scaling methods include \n",
    "z-score standardization or min-max scaling.\n",
    "\n",
    "6. **Data Preprocessing**: Proper data preprocessing, such as handling missing values and \n",
    "dealing with categorical features, can have a significant impact on KNN's performance. \n",
    "You might need to explore different techniques for data preprocessing depending on the\n",
    "nature of your dataset.\n",
    "\n",
    "To tune these hyperparameters and improve model performance, you can follow these steps:\n",
    "\n",
    "1. **Grid Search or Random Search**: Use techniques like grid search or random search to\n",
    "systematically explore different combinations of hyperparameters. This involves specifying\n",
    "a range of values for each hyperparameter and evaluating the model's performance for each combination.\n",
    "\n",
    "2. **Cross-Validation**: Use cross-validation to assess the model's performance on different \n",
    "subsets of the data. This helps ensure that your hyperparameter tuning results are robust \n",
    "and not influenced by the specific data split.\n",
    "\n",
    "3. **Validation Curve**: Plot validation curves to visualize how changing a specific\n",
    "hyperparameter affects the model's performance. This can help you identify the range \n",
    "of values that yield the best results.\n",
    "\n",
    "4. **Domain Knowledge**: Consider domain-specific knowledge when selecting hyperparameters.\n",
    "Some domain expertise might suggest certain choices for K, distance metric, or weighting scheme.\n",
    "\n",
    "5. **Iterative Process**: Hyperparameter tuning is often an iterative process. \n",
    "You may need to try multiple iterations, refining your hyperparameters based on \n",
    "previous results, until you achieve the desired level of performance.\n",
    "\n",
    "Remember that hyperparameter tuning should always be performed on a separate validation \n",
    "set or through cross-validation to avoid overfitting to the training data. Additionally,\n",
    "keep in mind that the optimal hyperparameters may vary depending on the specific dataset \n",
    "and problem you're working on, so it's crucial to experiment and adapt your choices accordingly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    The size of the training set can significantly affect the performance of a K-Nearest \n",
    "    Neighbors (KNN) classifier or regressor. Here's how:\n",
    "\n",
    "1. **Small Training Set**:\n",
    "   - **Overfitting**: With a small training set, KNN is more likely to overfit the data because\n",
    "it relies on local patterns in the training data. It may capture noise and not generalize well \n",
    "to new, unseen data.\n",
    "   - **Increased Variability**: The predictions can be highly variable since a small number of \n",
    "    nearest neighbors may not provide a representative sample of the data distribution.\n",
    "   - **Sensitive to Outliers**: Small datasets are more susceptible to the influence of outliers,\n",
    "leading to potentially skewed predictions.\n",
    "\n",
    "2. **Large Training Set**:\n",
    "   - **Better Generalization**: A larger training set can help KNN generalize better to unseen data, \n",
    "as it's more likely to capture the underlying distribution of the data.\n",
    "   - **Reduced Variance**: With more data points to consider, the predictions become less variable.\n",
    "   - **Robustness to Outliers**: Larger datasets tend to be more robust to outliers, as the impact\n",
    "of individual data points is diluted.\n",
    "\n",
    "To optimize the size of the training set for a KNN classifier or regressor, \n",
    "consider the following techniques:\n",
    "\n",
    "1. **Cross-Validation**: Use techniques like k-fold cross-validation to estimate the\n",
    "performance of your model on different training set sizes. This can help you identify \n",
    "the optimal trade-off between model performance and dataset size.\n",
    "\n",
    "2. **Learning Curves**: Plot learning curves that show how model performance\n",
    "(e.g., accuracy or mean squared error) changes with increasing training set size.\n",
    "This can help you visualize whether more data is likely to improve performance or \n",
    "if you're already reaching a plateau.\n",
    "\n",
    "3. **Data Augmentation**: If obtaining a larger dataset is challenging, consider data \n",
    "augmentation techniques to artificially increase the size of your training set. \n",
    "This involves creating new data points by applying transformations or perturbations to existing data.\n",
    "\n",
    "4. **Feature Engineering**: Carefully select and engineer relevant features to reduce the data's\n",
    "dimensionality and complexity. This can make KNN more effective with smaller datasets.\n",
    "\n",
    "5. **Feature Scaling**: Normalize or standardize your features to ensure that KNN's \n",
    "distance metric is not biased towards certain features. This can help\n",
    "the model perform better with limited data.\n",
    "\n",
    "6. **Feature Selection**: Identify and use the most informative features for your problem,\n",
    "discarding irrelevant or redundant ones. This can improve model performance with a smaller feature set.\n",
    "\n",
    "7. **Incremental Learning**: If obtaining a large labeled dataset is challenging, consider\n",
    "using transfer learning or semi-supervised learning techniques to leverage pre-trained \n",
    "models or unlabeled data.\n",
    "\n",
    "8. **Active Learning**: If labeling data is expensive, employ active learning strategies\n",
    "to select the most informative data points for labeling, thus maximizing the utility of\n",
    "a limited labeling budget.\n",
    "\n",
    "9. **Ensemble Methods**: Combine the predictions of multiple KNN models trained on different\n",
    "subsets of the data. Ensemble methods can help reduce the impact of noise and improve generalization.\n",
    "\n",
    "In summary, the size of the training set can impact the performance of KNN, and finding the\n",
    "right balance between dataset size and model complexity is crucial. Using techniques like\n",
    "cross-validation, learning curves, and data augmentation can help\n",
    "optimize the training set size for your specific problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    K-Nearest Neighbors (KNN) is a simple and intuitive machine learning algorithm, but\n",
    "    it has some potential drawbacks that can impact its performance. Here are some of the\n",
    "    drawbacks of using KNN as a classifier or regressor, along with strategies to overcome them:\n",
    "\n",
    "1. **Computational Complexity**: KNN can be computationally expensive, especially with \n",
    "large datasets. Calculating distances between the query point and all data points in\n",
    "the training set can be time-consuming.\n",
    "\n",
    "   * **Solution**: Use approximate nearest neighbor algorithms like Locality-Sensitive\n",
    "    Hashing (LSH) or KD-trees to speed up the search for nearest neighbors. You can also \n",
    "    reduce the dimensionality of the data using techniques like Principal Component Analysis (PCA)\n",
    "    to make the calculations faster.\n",
    "\n",
    "2. **Sensitive to Feature Scaling**: KNN is sensitive to the scale of features. Features with larger\n",
    "scales can dominate the distance metric, leading to biased results.\n",
    "\n",
    "   * **Solution**: Standardize or normalize the features to have zero mean and unit variance.\n",
    "    This ensures that all features contribute equally to the distance calculations.\n",
    "\n",
    "3. **Curse of Dimensionality**: In high-dimensional spaces, KNN may suffer from the curse \n",
    "of dimensionality. As the number of dimensions increases, the distance between data points\n",
    "becomes less meaningful, making it difficult to find meaningful neighbors.\n",
    "\n",
    "   * **Solution**: Feature selection or dimensionality reduction techniques can help reduce\n",
    "    the number of irrelevant or redundant features. Alternatively, consider using dimensionality\n",
    "    reduction techniques like PCA or t-SNE to project the data into a lower-dimensional space.\n",
    "\n",
    "4. **Choosing the Right K**: Selecting the optimal value of K (the number of neighbors to consider)\n",
    "can be challenging. A small K can make the model sensitive to noise, while\n",
    "a large K can make it overly biased.\n",
    "\n",
    "   * **Solution**: Perform hyperparameter tuning using techniques like cross-validation \n",
    "    to find the optimal value of K for your specific dataset. Consider using algorithms\n",
    "    like grid search or randomized search to automate this process.\n",
    "\n",
    "5. **Imbalanced Data**: KNN can be biased when dealing with imbalanced datasets,\n",
    "where one class significantly outnumbers the others. It may tend to classify\n",
    "new samples into the majority class.\n",
    "\n",
    "   * **Solution**: Use techniques like oversampling, undersampling, or synthetic data\n",
    "    generation to balance the dataset. Additionally, you can assign different weights\n",
    "    to different classes or use other advanced techniques like SMOTE\n",
    "    (Synthetic Minority Over-sampling Technique) to address class imbalance.\n",
    "\n",
    "6. **Local Optima**: KNN relies on local information, which means it might get \n",
    "stuck in local optima if the training data is not representative of the \n",
    "underlying data distribution.\n",
    "\n",
    "   * **Solution**: Ensure that your training dataset is large and diverse enough to capture\n",
    "    the underlying data distribution. You can also consider using other algorithms like \n",
    "    decision trees or ensemble methods to overcome this limitation.\n",
    "\n",
    "7. **Storage and Memory Requirements**: KNN stores the entire training dataset in memory,\n",
    "which can be memory-intensive for large datasets.\n",
    "\n",
    "   * **Solution**: Consider using approximate nearest neighbor methods or distributed \n",
    "    computing frameworks to handle large datasets efficiently.\n",
    "\n",
    "In summary, while KNN is a straightforward algorithm, it does have its limitations.\n",
    "Overcoming these drawbacks often involves preprocessing the data, selecting appropriate \n",
    "hyperparameters, and using complementary techniques to improve its performance\n",
    "on specific tasks. Additionally, for more complex problems, considering\n",
    "alternative machine learning algorithms may be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86182502-f941-435b-8038-feac61d40459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
