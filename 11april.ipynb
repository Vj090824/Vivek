{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5116aa-0bfe-41b0-b8ec-6b56e45f376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "        An ensemble technique in machine learning refers to the practice of combining \n",
    "multiple individual models (often called base models or weak learners) to create\n",
    "a more robust and accurate predictive model. The idea behind ensemble methods is that by combining \n",
    "the predictions of several models, you can often achieve better results than using any \n",
    "single model on its own. Ensemble techniques are widely used in machine learning because \n",
    "they can improve model generalization, reduce overfitting, and enhance predictive performance.\n",
    "\n",
    "There are several common ensemble techniques, including:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):** Bagging involves training multiple instances of the\n",
    "same base model on different subsets of the training data, typically by randomly sampling \n",
    "with replacement. The predictions from these models are then aggregated, often by taking a\n",
    "majority vote (for classification problems) or averaging (for regression problems). \n",
    "Random Forest is a popular ensemble method based on bagging.\n",
    "\n",
    "2. **Boosting:** Boosting is an iterative ensemble technique where base models are trained\n",
    "sequentially, and each subsequent model focuses on the examples that were misclassified \n",
    "by the previous models. Boosting algorithms like AdaBoost and Gradient Boosting \n",
    "are commonly used for this purpose.\n",
    "\n",
    "3. **Stacking:** Stacking, also known as stacked generalization, involves training\n",
    "multiple diverse base models, and then training a meta-model (often called a blender\n",
    "    or meta-learner) on the predictions of these base models. The meta-model learns how to \n",
    "combine the base models' outputs to make the final prediction.\n",
    "\n",
    "4. **Voting:** Voting ensembles combine the predictions of multiple base models by taking a \n",
    "majority vote (for classification) or averaging (for regression). There are different types\n",
    "of voting ensembles, such as hard voting (majority vote) and soft voting (weighted average).\n",
    "\n",
    "5. **Bootstrapped Ensembles:** These ensembles involve creating multiple datasets through\n",
    "bootstrapping (random sampling with replacement) and training a separate base model on each\n",
    "dataset. Then, the models' predictions are combined to make a final prediction.\n",
    "\n",
    "Ensemble techniques are powerful because they can improve the overall performance of machine\n",
    "learning models by reducing bias and variance, making them more robust and capable of handling\n",
    "complex datasets. Different ensemble methods may be more suitable for specific types of \n",
    "problems or datasets, and the choice of ensemble technique often depends on \n",
    "empirical testing and domain knowledge.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    Ensemble techniques are used in machine learning for several important reasons,\n",
    "    as they offer various advantages and help improve the overall performance and\n",
    "    robustness of predictive models. Here are some key reasons why ensemble\n",
    "    techniques are widely employed:\n",
    "\n",
    "1. **Improved Predictive Accuracy:** One of the primary motivations for using ensemble\n",
    "techniques is to enhance the predictive accuracy of machine learning models. Ensembles\n",
    "combine multiple base models (e.g., decision trees, neural networks, or any other algorithms) \n",
    "to produce a final prediction that is often more accurate than the individual base models.\n",
    "By aggregating the predictions of multiple models, ensembles can reduce the impact of overfitting\n",
    "and noise in the data, leading to better generalization to unseen data.\n",
    "\n",
    "2. **Reduced Variance and Bias:** Ensembles help in reducing both bias and variance in model\n",
    "predictions. Bias occurs when a model makes consistent but systematically wrong predictions,\n",
    "while variance refers to the model's sensitivity to small changes in the training data. \n",
    "Ensembles typically consist of diverse base models, and the combination of these models \n",
    "can help balance out bias and variance issues, resulting in more stable and reliable predictions.\n",
    "\n",
    "3. **Enhanced Robustness:** Ensembles are more robust to outliers and noisy data points \n",
    "compared to single models. Outliers can have a disproportionate influence on the predictions of \n",
    "individual models, but ensembles tend to be less affected by such anomalies since they \n",
    "aggregate predictions from multiple sources.\n",
    "\n",
    "4. **Model Generalization:** Ensemble techniques often improve a model's ability to generalize\n",
    "to different datasets. By combining models with different characteristics, ensembles can\n",
    "capture various patterns and relationships in the data, making them more adaptable to different scenarios.\n",
    "\n",
    "5. **Reduction of Overfitting:** Individual machine learning models may overfit the training data,\n",
    "capturing noise and irrelevant details. Ensembles mitigate this risk by combining models that \n",
    "may overfit in different ways, making it less likely for all of them to make the same overfit predictions.\n",
    "\n",
    "6. **Increased Stability:** Ensembles can provide more stable and consistent predictions across \n",
    "different runs or subsets of data. This stability is valuable in real-world\n",
    "applications where consistency is crucial.\n",
    "\n",
    "7. **Handling Complex Relationships:** In cases where the underlying relationships in the data\n",
    "are complex and difficult to model with a single algorithm, ensembles can capture these intricate\n",
    "patterns by combining simpler models. This makes them effective for tasks like image recognition,\n",
    "natural language processing, and other complex data analysis problems.\n",
    "\n",
    "8. **Reduction of Model Selection Uncertainty:** Ensembles can reduce the uncertainty associated\n",
    "with choosing a single best-performing model by combining multiple models, thus making\n",
    "it less critical to select the \"perfect\" algorithm or hyperparameters.\n",
    "\n",
    "Popular ensemble techniques include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost,\n",
    "Gradient Boosting), and random forests, among others. Each of these techniques has its own \n",
    "characteristics and is suitable for different types of problems, but they all leverage\n",
    "the power of combining multiple models to improve overall performance and\n",
    "robustness in machine learning applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is bagging?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    Bagging, which stands for Bootstrap Aggregating, is an ensemble machine\n",
    "    learning technique used to improve the accuracy and robustness of predictive models\n",
    "    , especially in the context of decision trees and other high-variance models. \n",
    "    It was introduced by Leo Breiman in the 1990s.\n",
    "\n",
    "The main idea behind bagging is to create multiple subsets (or bags) of the training data\n",
    "through a process called bootstrapping. Bootstrapping involves randomly selecting samples\n",
    "from the original training dataset with replacement, which means that some data points may\n",
    "be selected multiple times, while others may not be selected at all. These subsets \n",
    "are used to train multiple base models independently.\n",
    "\n",
    "After training these base models, bagging combines their predictions to make a\n",
    "final prediction. For classification tasks, bagging often uses majority voting,\n",
    "where each base model's prediction is counted, and the class with the most votes \n",
    "is chosen as the final prediction. For regression tasks, bagging typically takes\n",
    "the average of the base models' predictions.\n",
    "\n",
    "Bagging offers several benefits:\n",
    "\n",
    "1. **Reduced Variance:** By training multiple models on different subsets of data,\n",
    "bagging helps reduce the variance of the final prediction. This means the ensemble\n",
    "model is less likely to overfit the training data.\n",
    "\n",
    "2. **Improved Accuracy:** On average, the ensemble model's performance is often better\n",
    "than that of individual base models because it combines their strengths\n",
    "and mitigates their weaknesses.\n",
    "\n",
    "3. **Robustness:** Bagging is less sensitive to outliers and noisy data points\n",
    "since they may not appear in every bootstrapped subset.\n",
    "\n",
    "Random Forests, one of the most popular ensemble algorithms, are a specific \n",
    "application of bagging where the base models are decision trees. In addition\n",
    "to bagging, Random Forests introduce randomness during the tree construction\n",
    "process by considering only a random subset of features at each split,\n",
    "further enhancing the model's diversity and performance.\n",
    "\n",
    "In summary, bagging is a powerful technique for improving the stability and accuracy \n",
    "of machine learning models by creating an ensemble of base models trained on \n",
    "different subsets of the data and combining their predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What is boosting?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    Boosting is a machine learning ensemble technique used for improving the predictive \n",
    "    performance of a model, typically a decision tree or a weak learner. It works by\n",
    "combining multiple weak models (often referred to as \"base learners\" or \"weak classifiers\") \n",
    "to create a strong predictive model.\n",
    "\n",
    "Here's how boosting generally works:\n",
    "\n",
    "1. **Initialization**: Initially, each training data point is assigned an equal weight.\n",
    "\n",
    "2. **Base Learner Training**: A weak learner (e.g., a simple decision tree) is trained on\n",
    "the data with these initial weights. The weak learner's task is to predict the target variable.\n",
    "\n",
    "3. **Weighted Errors**: After training the weak learner, the model calculates the errors it made.\n",
    "Data points that were incorrectly classified by the weak learner are given more weight, making\n",
    "them more important for the next round of training.\n",
    "\n",
    "4. **Iterative Process**: Boosting is an iterative process, and the above steps are repeated \n",
    "multiple times. In each iteration, a new weak learner is trained, and the weights of \n",
    "data points are updated based on the errors made in the previous round.\n",
    "\n",
    "5. **Combining Weak Learners**: The final prediction is made by combining the predictions\n",
    "of all the weak learners. Each weak learner's prediction is weighted based on its\n",
    "performance in the training process. Typically, weak learners with better performance have higher weights.\n",
    "\n",
    "The key idea behind boosting is that it focuses on the data points that are hard to classify\n",
    "correctly. By assigning higher weights to these data points in each iteration, boosting aims \n",
    "to improve the model's performance on the difficult-to-classify examples. This iterative process\n",
    "continues until a predefined number of iterations is reached or until\n",
    "a certain level of accuracy is achieved.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting,\n",
    "and XGBoost. These algorithms have variations and improvements that make them effective \n",
    "for various types of machine learning tasks. Boosting is known for its ability to produce \n",
    "highly accurate models and is widely used in applications like classification and regression.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    Ensemble techniques are machine learning methods that combine multiple models to\n",
    "    improve predictive performance and overall robustness. There are several benefits\n",
    "    to using ensemble techniques:\n",
    "\n",
    "1. **Improved Predictive Accuracy:** One of the primary advantages of ensemble methods\n",
    "is that they often lead to better predictive accuracy compared to individual models.\n",
    "By combining the predictions of multiple models, ensembles can reduce errors and \n",
    "make more accurate predictions.\n",
    "\n",
    "2. **Reduction in Overfitting:** Ensembles can help reduce overfitting, which occurs\n",
    "when a model performs well on the training data but poorly on unseen data. Combining\n",
    "multiple models with different characteristics or training data subsets can help mitigate overfitting.\n",
    "\n",
    "3. **Enhanced Robustness:** Ensembles are less sensitive to noise in the data and\n",
    "are more robust in handling outliers and anomalies. When different models make \n",
    "different errors, the ensemble can often make more robust predictions.\n",
    "\n",
    "4. **Model Stability:** Ensembles can provide more stable and consistent predictions\n",
    "compared to individual models, making them suitable for critical applications\n",
    "where consistency is important.\n",
    "\n",
    "5. **Versatility:** Ensemble techniques can be applied to a wide range of machine learning\n",
    "algorithms and models, including decision trees, random forests, gradient boosting, and more.\n",
    "This versatility allows you to choose the best combination of models for your specific problem.\n",
    "\n",
    "6. **Feature Selection:** Some ensemble methods, such as Random Forests, provide a measure\n",
    "of feature importance. This can help in identifying the most relevant features in your dataset,\n",
    "aiding in feature selection and dimensionality reduction.\n",
    "\n",
    "7. **Bias Reduction:** Ensembles can help reduce bias in model predictions. By combining\n",
    "models trained on different subsets of data or using different algorithms, ensembles can\n",
    "provide a more balanced and less biased view of the data.\n",
    "\n",
    "8. **Handling Imbalanced Data:** Ensembles can be effective in handling imbalanced \n",
    "datasets, where one class has significantly fewer samples than others. By combining \n",
    "multiple models, ensembles can give more weight to the minority class,\n",
    "improving classification performance.\n",
    "\n",
    "9. **Interpretability:** Some ensemble techniques, such as bagging with decision trees, \n",
    "can provide insights into feature importance and model behavior,\n",
    "making it easier to interpret and explain the model's predictions.\n",
    "\n",
    "10. **Flexibility:** Ensembles can be adapted to different problem types,\n",
    "including classification, regression, and even more complex tasks like anomaly\n",
    "detection and recommendation systems.\n",
    "\n",
    "Common ensemble techniques include Bagging, Boosting, Stacking, and Random Forests,\n",
    "among others. The choice of ensemble method depends on the specific problem, the data,\n",
    "and the algorithms you are working with. However, in many cases, ensembles can significantly\n",
    "improve the performance and reliability of machine learning models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    Ensemble techniques are not always better than individual models; their effectiveness \n",
    "    depends on various factors and the specific problem you are trying to solve. Ensemble \n",
    "    techniques work by combining the predictions of multiple individual models to improve\n",
    "    overall performance, and they are often used to increase predictive accuracy, reduce \n",
    "    overfitting, and enhance model robustness. However, there are situations where ensemble\n",
    "    methods may not provide significant benefits or could even perform worse than \n",
    "    individual models. Here are some key considerations:\n",
    "\n",
    "1. **Quality of Base Models**: The performance of an ensemble largely depends on the\n",
    "quality and diversity of the base models. If the individual models in the ensemble \n",
    "are weak or highly correlated, the ensemble may not yield significant improvements.\n",
    "\n",
    "2. **Diversity of Models**: Ensembles tend to work better when the base models are diverse\n",
    "in terms of algorithms, data subsets, or feature representations. If the ensemble consists\n",
    "of similar models, it may not provide much benefit.\n",
    "\n",
    "3. **Size of the Dataset**: In cases where you have a very small dataset, creating diverse\n",
    "base models for an ensemble may be challenging. In such situations, a single \n",
    "well-tuned model might perform better.\n",
    "\n",
    "4. **Computational Resources**: Building and training an ensemble of models can be\n",
    "computationally expensive and time-consuming. In some cases, using a single \n",
    "model may be more practical and efficient.\n",
    "\n",
    "5. **Interpretability**: Ensembles are often more complex than individual models,\n",
    "which can make them harder to interpret. In scenarios where interpretability is crucial,\n",
    "a single model might be preferred.\n",
    "\n",
    "6. **Overhead**: Ensembles come with additional overhead in terms of model combination\n",
    "and maintenance. If the problem doesn't require the extra effort, a single model may suffice.\n",
    "\n",
    "7. **Domain Knowledge**: Sometimes, domain-specific knowledge can lead to the development of \n",
    "a highly effective single model. In such cases, an ensemble may not be necessary.\n",
    "\n",
    "8. **Time Constraints**: If there are strict time constraints for making predictions, an\n",
    "ensemble might not be practical due to the extra time required for combining predictions.\n",
    "\n",
    "In summary, ensemble techniques are a powerful tool in machine learning, but they\n",
    "should be chosen judiciously based on the characteristics of the problem,\n",
    "the quality of base models, and the available resources. It's essential \n",
    "to experiment and evaluate the performance of both individual models and \n",
    "ensembles to determine which approach works best for a specific task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    In statistics, a confidence interval is a range of values that is likely to contain\n",
    "    the true population parameter of interest with a certain level of confidence.\n",
    "    Bootstrap is a resampling technique used to estimate the sampling distribution\n",
    "    of a statistic and to calculate confidence intervals without making strong \n",
    "    parametric assumptions about the underlying population distribution. Here's how\n",
    "    you can calculate a confidence interval using the bootstrap method:\n",
    "\n",
    "1. **Collect your sample data:** Start by collecting a sample of data from the population\n",
    "you are interested in studying. This sample should be representative of the population.\n",
    "\n",
    "2. **Resampling:** Perform resampling with replacement from your original sample to create\n",
    "a large number of bootstrap samples. Each bootstrap sample should have the same size as your\n",
    "original sample. This process is often repeated thousands or even tens of thousands of times.\n",
    "\n",
    "3. **Calculate the statistic:** For each bootstrap sample, calculate the statistic of interest.\n",
    "This statistic could be a mean, median, variance, correlation coefficient, or any\n",
    "other parameter you want to estimate.\n",
    "\n",
    "4. **Construct the bootstrap sampling distribution:** You now have a collection of statistics\n",
    "from the bootstrap samples. This collection forms the bootstrap sampling\n",
    "distribution of the statistic. You can use this distribution to approximate \n",
    "the sampling variability of the statistic.\n",
    "\n",
    "5. **Calculate the confidence interval:** To construct a confidence interval,\n",
    "you need to determine the range of values within which your statistic is likely to fall\n",
    "with a certain level of confidence. The confidence interval is typically centered around \n",
    "the sample statistic (e.g., the sample mean) and is determined by the percentiles\n",
    "of the bootstrap sampling distribution.\n",
    "\n",
    "   - **Percentile Method:** Calculate the desired confidence level (e.g., 95%) and\n",
    "    find the lower and upper percentiles of the bootstrap sampling distribution that\n",
    "    correspond to the desired confidence level. These percentiles define the lower and\n",
    "    upper bounds of the confidence interval.\n",
    "\n",
    "   - **Basic Bootstrap Confidence Interval:** A common method is to use the empirical\n",
    "quantiles of the bootstrap distribution. The lower and upper bounds of the confidence\n",
    "interval are the α/2 and 1-α/2 quantiles, respectively, where α is your chosen\n",
    "significance level (e.g., 0.05 for a 95% confidence interval).\n",
    "\n",
    "6. **Report the confidence interval:** Finally, report the calculated confidence\n",
    "interval as an estimate of the population parameter. For example, you might say,\n",
    "\"We are 95% confident that the true population mean falls within the range\n",
    "[lower bound, upper bound].\"\n",
    "\n",
    "The bootstrap method allows you to estimate confidence intervals for various \n",
    "statistics without assuming a specific distribution for your data, making it a\n",
    "valuable tool in statistics and data analysis. The precision of your confidence\n",
    "interval depends on the number of bootstrap samples you generate;\n",
    "more samples generally lead to more accurate intervals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "\n",
    "    Bootstrap is a resampling technique used in statistics and machine learning to estimate\n",
    "    the sampling distribution of a statistic by repeatedly resampling from the observed data.\n",
    "    It's a powerful tool for making inferences about a population when you have a\n",
    "    limited sample size. The primary idea behind bootstrap is to create multiple \"pseudo\" sample\n",
    "    s from the original data, which allows you to approximate the distribution of\n",
    "    a statistic or parameter of interest.\n",
    "\n",
    "Here are the steps involved in the bootstrap method:\n",
    "\n",
    "1. **Data Collection**: Start with your original dataset, which is typically a \n",
    "sample from a larger population.\n",
    "\n",
    "2. **Resampling**: Randomly draw, with replacement, a sample of the same size as the original\n",
    "dataset from the original data. This creates a \"bootstrap sample,\" which might \n",
    "contain some of the same data points multiple times and omit others.\n",
    "\n",
    "3. **Statistic Calculation**: Calculate the statistic of interest (e.g., mean, median,\n",
    "standard deviation, regression coefficients, etc.) on the bootstrap sample.\n",
    "This statistic is an estimate of the parameter you're interested in.\n",
    "\n",
    "4. **Repeat**: Repeat steps 2 and 3 a large number of times (typically thousands or more)\n",
    "to create a distribution of the statistic of interest. Each iteration generates a new estimate of the parameter.\n",
    "\n",
    "5. **Estimate the Sampling Distribution**: Analyze the distribution of the calculated statistics.\n",
    "You can use this distribution to make inferences about the population parameter,\n",
    "such as estimating its mean, standard error, confidence intervals, or even making hypothesis tests.\n",
    "\n",
    "6. **Summary Statistics**: Compute summary statistics on the distribution, such as the mean,\n",
    "standard error, and percentiles, to understand the uncertainty associated with the parameter estimate.\n",
    "\n",
    "7. **Visualization**: Often, it's helpful to visualize the bootstrap distribution using histograms,\n",
    "density plots, or confidence interval plots to gain insights into the parameter's variability.\n",
    "\n",
    "Bootstrap has several advantages:\n",
    "\n",
    "- It doesn't rely on any specific assumptions about the underlying population\n",
    "distribution, making it non-parametric and robust.\n",
    "- It can be applied to various statistical problems, including estimating parameters,\n",
    "constructing confidence intervals, and hypothesis testing.\n",
    "- It provides a way to quantify the uncertainty associated with parameter estimates.\n",
    "\n",
    "However, there are some caveats:\n",
    "\n",
    "- Bootstrap assumes that the original sample is representative of the population.\n",
    "- It can be computationally intensive when applied to large datasets or complex models.\n",
    "- The results can be sensitive to the number of bootstrap resamples, so choosing an \n",
    "appropriate number is essential.\n",
    "\n",
    "In summary, the bootstrap method is a powerful and versatile technique for estimating\n",
    "the sampling distribution of a statistic or parameter, allowing statisticians and\n",
    "data scientists to make more robust and reliable inferences from limited data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    To estimate the 95% confidence interval for the population mean height using the bootstrap method, you can follow these steps:\n",
    "\n",
    "1. **Collect your sample:** You have already collected a sample of 50 trees with a mean height of 15 meters and a standard deviation of 2 meters.\n",
    "\n",
    "2. **Generate Bootstrap Samples:** Create a large number of bootstrap samples by randomly resampling (with replacement) from your original sample. Each bootstrap sample should also contain 50 tree height measurements.\n",
    "\n",
    "3. **Calculate the Mean for Each Bootstrap Sample:** Calculate the mean height for each of the bootstrap samples.\n",
    "\n",
    "4. **Construct the Confidence Interval:** Sort the bootstrap sample means and find the 2.5th\n",
    "percentile and the 97.5th percentile of the distribution of bootstrap means. \n",
    "These percentiles correspond to the lower and upper bounds of the 95% confidence interval.\n",
    "\n",
    "Here's how you can calculate it in Python:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "sample_mean = 15  # Mean of the sample\n",
    "sample_std = 2    # Standard deviation of the sample\n",
    "sample_size = 50  # Size of the sample\n",
    "num_bootstrap_samples = 10000  # Number of bootstrap samples\n",
    "\n",
    "# Create an array to store bootstrap sample means\n",
    "bootstrap_means = []\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Generate a bootstrap sample by resampling with replacement\n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_std, sample_size)\n",
    "    # Calculate the mean of the bootstrap sample and store it\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Sort the bootstrap sample means\n",
    "bootstrap_means.sort()\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "lower_bound = bootstrap_means[int(0.025 * num_bootstrap_samples)]\n",
    "upper_bound = bootstrap_means[int(0.975 * num_bootstrap_samples)]\n",
    "\n",
    "print(f\"95% Confidence Interval: ({lower_bound:.2f}, {upper_bound:.2f}) meters\")\n",
    "\n",
    "\n",
    "This code generates 10,000 bootstrap samples, calculates the mean for each bootstrap\n",
    "sample, and then finds the 2.5th and 97.5th percentiles of the bootstrap means to construct\n",
    "the 95% confidence interval for the population mean height.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
