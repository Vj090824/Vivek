{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95d82c-c142-4208-93de-3445906d96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
    "\n",
    "Given a set of labeled training data points:\n",
    "- Input vectors: x1, x2,....,xn\n",
    "- Corresponding labels: y1, y2,...,yn, where yi ∈ {-1, 1}\n",
    "\n",
    "The goal of a linear SVM is to find a hyperplane in the feature space that\n",
    "best separates the data into two classes while maximizing the margin.\n",
    "The equation of this hyperplane can be expressed as:\n",
    "\n",
    "    \n",
    "w⋅x+b=0\n",
    "Where:\n",
    "- w is a vector of weights (coefficients) that defines the orientation of the hyperplane.\n",
    "- x is the input feature vector.\n",
    "- b is the bias term or intercept, which determines the position of the hyperplane.\n",
    "\n",
    "The decision function of the linear SVM can be defined as:\n",
    "\n",
    "    f(x)= w⋅x+b\n",
    "\n",
    "The SVM aims to find (w) and (b) such that the following conditions hold:\n",
    "1. f(xi) ≥ 1 for all data points xi with label yi = 1 (positive class).\n",
    "2. f(xi) ≤ -1 for all data points xi with label yi = -1 (negative class).\n",
    "3. The margin between the two parallel hyperplanes f(x) = 1 and f(x) = -1 is maximized.\n",
    "\n",
    "The margin is defined as the distance between these two hyperplanes, and the SVM aims \n",
    "to maximize this margin while satisfying the conditions above. The vector w  is \n",
    "perpendicular to the hyperplane, and its magnitude (∥w∥) is inversely proportional\n",
    "to the margin. Therefore, the optimization problem for finding the best hyperplane is \n",
    "often formulated as a convex quadratic programming problem, where the objective is to \n",
    "maximize 1∥w∥2 subject to the constraints mentioned above.\n",
    "\n",
    "The SVM then makes predictions by evaluating the sign of \\(f(x)\\). If f(x) ≥ 0,\n",
    "the input x is classified as the positive class (1); otherwise, \n",
    "it is classified as the negative class (-1).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    The objective function of a linear Support Vector Machine (SVM) is to find the hyperplane\n",
    "    that best separates the data into two classes while maximizing the margin between the two\n",
    "    classes. In a binary classification problem, where you have two classes, typically\n",
    "    labeled as +1 and -1, the objective function is to find a hyperplane defined by the equation:\n",
    "\n",
    "w · x + b = 0\n",
    "\n",
    "Where:\n",
    "- \"w\" is the weight vector perpendicular to the hyperplane.\n",
    "- \"x\" is the input data point.\n",
    "- \"b\" is the bias term or the intercept.\n",
    "\n",
    "The goal of the linear SVM is to find the \"w\" and \"b\" that maximize the margin\n",
    "between the two classes. The margin is defined as the distance between the hyperplane\n",
    "and the nearest data point from each class. Mathematically, the margin is given by:\n",
    "\n",
    "Margin = 2 / ||w||\n",
    "\n",
    "Here, \"||w||\" represents the Euclidean norm of the weight vector \"w.\"\n",
    "\n",
    "The objective function can be formulated as an optimization problem, typically a\n",
    "quadratic programming problem, as follows:\n",
    "\n",
    "Minimize: 1/2 * ||w||^2\n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "y_i * (w · x_i + b) ≥ 1 for all training data points (x_i, y_i), where y_i\n",
    "is +1 or -1 depending on the class label.\n",
    "\n",
    "In this formulation, the SVM aims to minimize the L2 norm of the weight vector\n",
    "\"w\" while ensuring that all data points are correctly classified and are at least \n",
    "at a distance of 1 from the hyperplane. This constraint ensures that the margin is maximized.\n",
    "\n",
    "The linear SVM finds the optimal \"w\" and \"b\" values that satisfy these\n",
    "constraints and minimize the objective function, resulting in a hyperplane\n",
    "that effectively separates the two classes with a maximum margin.\n",
    "This approach makes SVMs particularly effective for binary classification\n",
    "tasks with a clear margin of separation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    The kernel trick is a fundamental concept in Support Vector Machines (SVMs) that\n",
    "    allows SVMs to perform non-linear classification or regression tasks by implicitly \n",
    "    mapping the input data into a higher-dimensional space. This trick makes it possible\n",
    "    for SVMs to find a hyperplane (or decision boundary)\n",
    "    that can separate the data even when the data is not linearly separable in\n",
    "    the original feature space.\n",
    "\n",
    "Here's how the kernel trick works:\n",
    "\n",
    "1. Original Feature Space: In a typical SVM, you start with your original feature space,\n",
    "where your data points are represented as vectors in a lower-dimensional space.\n",
    "\n",
    "2. Non-Linear Mapping: The kernel trick involves applying a mathematical function called a\n",
    "\"kernel\" to map the data from the original feature space to a higher-dimensional feature space.\n",
    "This mapping is typically non-linear and allows for more complex relationships to be captured.\n",
    "\n",
    "3. Linear Separation: In this higher-dimensional space, SVM tries to find a hyperplane that\n",
    "best separates the data points into different classes while maximizing the margin (distance)\n",
    "between the classes. This is done as if the data were linearly separable in this higher-dimensional space.\n",
    "\n",
    "4. Implicit Calculation: The clever part of the kernel trick is that it doesn't\n",
    "explicitly compute the transformation into the higher-dimensional space, which\n",
    "would be computationally expensive for large datasets or complex transformations. \n",
    "Instead, it relies on a kernel function that computes the dot product between pairs \n",
    "of data points in the higher-dimensional space without explicitly calculating the\n",
    "transformation. Common kernel functions include the linear kernel, polynomial kernel,\n",
    "and radial basis function (RBF) kernel, among others.\n",
    "\n",
    "By using the kernel trick, SVMs can effectively learn complex decision boundaries\n",
    "in the original feature space without explicitly representing the data in the \n",
    "higher-dimensional space, making them powerful tools for both linear and\n",
    "non-linear classification and regression tasks. Different kernel functions \n",
    "are chosen based on the specific characteristics of the data and the problem at hand.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "In Support Vector Machines (SVM), support vectors are the data points that play a \n",
    "crucial role in defining the decision boundary (hyperplane) between different classes.\n",
    "SVM is a supervised machine learning algorithm used for classification and regression tasks,\n",
    "and it works by finding the optimal hyperplane that maximizes the margin between classes.\n",
    "Support vectors are the data points closest to this hyperplane, and they are the ones that\n",
    "are most influential in determining the position and orientation of the hyperplane.\n",
    "\n",
    "Here's an explanation of the role of support vectors in SVM with an example:\n",
    "\n",
    "**Example: Binary Classification**\n",
    "\n",
    "Let's say you have a binary classification problem where you want to classify emails\n",
    "as either spam or non-spam based on two features: the number of words in the email\n",
    "and the number of links in the email. You have a dataset with labeled examples.\n",
    "\n",
    "1. **Data Preparation**: Your dataset consists of several email examples, and you\n",
    "represent them as points in a two-dimensional feature space (number of words, number of links).\n",
    "\n",
    "2. **Training SVM**: When you train an SVM on this data, the algorithm's objective is \n",
    "to find the hyperplane that best separates the two classes (spam and non-spam)\n",
    "while maximizing the margin between them. The margin is the distance between the \n",
    "hyperplane and the nearest data points from each class.\n",
    "\n",
    "3. **Support Vectors**: Support vectors are the data points from each class that lie\n",
    "closest to the decision boundary. These are the data points that are the most challenging\n",
    "to classify correctly and have the smallest margin. They are essentially the \"support\" \n",
    "for the decision boundary. In other words, if you were to move or remove any of these\n",
    "support vectors, the position and orientation of the hyperplane would change.\n",
    "\n",
    "4. **Optimal Hyperplane**: The hyperplane found by the SVM is positioned in such\n",
    "a way that it maximizes the margin between the support vectors of the two classes. \n",
    "This results in the most robust decision boundary that generalizes well to unseen data.\n",
    "\n",
    "5. **Classification**: When you want to classify a new email as spam or non-spam, \n",
    "you can do so by checking on which side of the hyperplane the new data point falls.\n",
    "If it's on the same side as the support vectors of the spam class, it's classified\n",
    "as spam; otherwise, it's classified as non-spam.\n",
    "\n",
    "In summary, support vectors are the data points that are closest to the decision boundary \n",
    "in an SVM. They are critical because they define the position and orientation of the\n",
    "hyperplane, which, in turn, determines the algorithm's ability to classify new, \n",
    "unseen data accurately. SVM aims to maximize the margin between these support\n",
    "vectors to create a robust classifier. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    Support Vector Machines (SVM) are a class of supervised machine learning algorithms used\n",
    "    for classification and regression tasks. They work by finding a hyperplane that best \n",
    "    separates the data into different classes. In SVM, there are different concepts related to hyperplanes \n",
    "    and margins, including the hyperplane itself, marginal plane, soft margin, and hard margin. Let's\n",
    "    illustrate these concepts with examples and graphs.\n",
    "\n",
    "1. **Hyperplane**:\n",
    "   - A hyperplane is a decision boundary that separates data points of one class from\n",
    "another in a binary classification problem.\n",
    "   - In a 2D space, a hyperplane is a straight line. In higher dimensions, it's \n",
    "    a flat affine subspace.\n",
    "   - The equation of a hyperplane in 2D is given by: `w0 + w1*x1 + w2*x2 = 0`, where \n",
    "`w0`, `w1`, and `w2` are the coefficients of the hyperplane, and `x1` and `x2` are the features.\n",
    "\n",
    "   Example:\n",
    "   Consider a simple 2D classification problem with two classes (blue and red points).\n",
    "The hyperplane that separates the two classes is shown in the graph below as a straight line.\n",
    "\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# we create 70 separable points\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=6)\n",
    "\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel=\"linear\", C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    plot_method=\"contour\",\n",
    "    colors=\"k\",\n",
    "    levels=[-1, 0, 1],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"],\n",
    "    ax=ax,\n",
    ")\n",
    "# plot support vectors\n",
    "ax.scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=100,\n",
    "    linewidth=1,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.show() \n",
    "   \n",
    " \n",
    "    \n",
    "    \n",
    "2. **Marginal Plane**:\n",
    "   - The marginal plane in SVM is the hyperplane that is closest to the data points of both\n",
    "classes, ensuring a maximum margin between the hyperplane and the data points.\n",
    "   - This plane is important because it defines the margin, which is the distance between \n",
    "    the marginal plane and the nearest data point (support vector).\n",
    "\n",
    "   Example:\n",
    "   In the same 2D problem, the marginal plane is the one that maximizes the distance between\n",
    "itself and the nearest data point from each class. The support vectors \n",
    "(points closest to the marginal plane) are shown as bold points in the graph below:\n",
    "\n",
    "   \n",
    "3. **Hard Margin SVM**:\n",
    "   - In a hard margin SVM, the goal is to find a hyperplane that perfectly separates \n",
    "the two classes without any misclassification.\n",
    "   - This is only possible when the data is linearly separable, meaning a hyperplane\n",
    "    can completely separate the two classes without errors.\n",
    "Example:\n",
    "A  hard margin SVM with linearly separable data.\n",
    "The hyperplane perfectly separates the two classes, and there are no misclassified points:\n",
    "\n",
    "\n",
    "4. **Soft Margin SVM**:\n",
    "   - In cases where the data is not linearly separable or when we want to allow some\n",
    "misclassification, we use a soft margin SVM.\n",
    "   - Soft margin SVM introduces a parameter (C) that controls the trade-off between \n",
    "    maximizing the margin and allowing some misclassification. A smaller C allows \n",
    "    more misclassification but a wider margin, while a larger C allows\n",
    "    fewer misclassifications but a narrower margin.\n",
    "\n",
    "   Example:\n",
    "    The data is not linearly separable, so a hard margin SVM is not feasible.\n",
    "    Instead, a soft margin SVM is used with a margin that allows for some misclassification:\n",
    "\n",
    "   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "# we create 80 separable points\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(80, 2) - [2, 2], np.random.randn(80, 2) + [2, 2]]\n",
    "Y = [0] * 80 + [1] * 80\n",
    "\n",
    "# figure number\n",
    "fignum = 1\n",
    "\n",
    "# fit the model\n",
    "for name, penalty in ((\"unreg\", 1), (\"reg\", 0.05)):\n",
    "    clf = svm.SVC(kernel=\"linear\", C=penalty)\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(-5, 5)\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "    # plot the parallels to the separating hyperplane that pass through the\n",
    "    # support vectors (margin away from hyperplane in direction\n",
    "    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n",
    "    # 2-d.\n",
    "    margin = 1 / np.sqrt(np.sum(clf.coef_**2))\n",
    "    yy_down = yy - np.sqrt(1 + a**2) * margin\n",
    "    yy_up = yy + np.sqrt(1 + a**2) * margin\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    plt.plot(xx, yy, \"k-\")\n",
    "    plt.plot(xx, yy_down, \"k--\")\n",
    "    plt.plot(xx, yy_up, \"k--\")\n",
    "\n",
    "    plt.scatter(\n",
    "        clf.support_vectors_[:, 0],\n",
    "        clf.support_vectors_[:, 1],\n",
    "        s=80,\n",
    "        facecolors=\"none\",\n",
    "        zorder=10,\n",
    "        edgecolors=\"k\",\n",
    "        cmap=plt.get_cmap(\"RdBu\"),\n",
    "    )\n",
    "    plt.scatter(\n",
    "        X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.get_cmap(\"RdBu\"), edgecolors=\"k\"\n",
    "    )\n",
    "\n",
    "    plt.axis(\"tight\")\n",
    "    x_min = -4.8\n",
    "    x_max = 4.2\n",
    "    y_min = -6\n",
    "    y_max = 6\n",
    "\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # Put the result into a contour plot\n",
    "    plt.contourf(XX, YY, Z, cmap=plt.get_cmap(\"RdBu\"), alpha=0.5, linestyles=[\"-\"])\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    fignum = fignum + 1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "In summary, SVM aims to find a hyperplane that separates data into different classes.\n",
    "The marginal plane defines the maximum margin between the hyperplane and the support\n",
    "vectors. Hard margin SVM requires perfect separation, while soft margin SVM allows \n",
    "some misclassification with a trade-off controlled by the parameter C.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    To implement SVM on the Iris dataset using scikit-learn, \n",
    "    you can follow these steps. First, make sure you have scikit-learn installed. You can install it using pip \n",
    "    if you don't have it already:\n",
    "    This code does the following:\n",
    "\n",
    "Loads the Iris dataset from scikit-learn.\n",
    "\n",
    "Splits the dataset into a training set and a testing set (80% training, 20% testing).\n",
    "\n",
    "Trains a linear SVM classifier on the training set.\n",
    "\n",
    "Predicts labels for the testing set and computes the accuracy of the model.\n",
    "\n",
    "Plots the decision boundaries using the first two features (Sepal length and Sepal width).\n",
    "\n",
    "You can experiment with different values of the regularization parameter C to see \n",
    "how it affects the model's performance. Adjust the C parameter in the SVC constructor \n",
    "to test different values.\n",
    "A smaller C value allows for more margin violations and may \n",
    "result in a simpler model, while a larger C value enforces a stricter margin\n",
    "and may lead to a more complex model.\n",
    "\n",
    "pip install scikit-learn\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "def train_svm_classifier(C):\n",
    "    svm_classifier = SVC(kernel='linear', C=C)\n",
    "    svm_classifier.fit(X_train[:, [0, 1]], y_train)\n",
    "    return svm_classifier\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "def predict_labels(svm_classifier, X_test):\n",
    "    return svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Plot the decision boundaries using two of the features\n",
    "def plot_decision_boundaries(svm_classifier, X, y, feature_indices, C):\n",
    "    h = .02  # step size in the mesh\n",
    "\n",
    "    x_min, x_max = X[:, feature_indices[0]].min() - 1, X[:, feature_indices[0]].max() + 1\n",
    "    y_min, y_max = X[:, feature_indices[1]].min() - 1, X[:, feature_indices[1]].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, feature_indices[0]], X[:, feature_indices[1]], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel(iris.feature_names[feature_indices[0]])\n",
    "    plt.ylabel(iris.feature_names[feature_indices[1]])\n",
    "    plt.title(f\"SVM Decision Boundaries (C={C})\")\n",
    "    plt.show()\n",
    "\n",
    "# Try different values of the regularization parameter C\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for C in C_values:\n",
    "    svm_classifier = train_svm_classifier(C)\n",
    "    y_pred = predict_labels(svm_classifier, X_test[:, [0, 1]])\n",
    "    accuracy = compute_accuracy(y_test, y_pred)\n",
    "    print(f\"Accuracy (C={C}): {accuracy:.2f}\")\n",
    "    plot_decision_boundaries(svm_classifier, X_train, y_train, [0, 1], C)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    Now, let's implement a basic linear SVM from scratch and compare it with scikit-learn's SVM:\n",
    "    pip install scikit-learn\n",
    "\n",
    "    import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a simple dataset for binary classification\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Custom Linear SVM Implementation\n",
    "class LinearSVM:\n",
    "    def __init__(self, lr=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(X[i], self.w) - self.b) >= 1:\n",
    "                    self.w -= self.lr * (2 * 1 / self.epochs * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * 1 / self.epochs * self.w - np.dot(X[i], y[i]))\n",
    "                    self.b -= self.lr * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) - self.b)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Train the custom SVM classifier\n",
    "custom_svm = LinearSVM()\n",
    "custom_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the custom SVM classifier\n",
    "y_pred_custom = custom_svm.predict(X_test)\n",
    "\n",
    "# Train and predict using scikit-learn's SVM classifier\n",
    "sklearn_svm = SVC(kernel='linear')\n",
    "sklearn_svm.fit(X_train, y_train)\n",
    "y_pred_sklearn = sklearn_svm.predict(X_test)\n",
    "\n",
    "# Compare accuracy\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"Custom SVM Accuracy: {accuracy_custom}\")\n",
    "print(f\"Scikit-learn SVM Accuracy: {accuracy_sklearn}\")\n",
    "\n",
    "\n",
    "This code creates a simple dataset, implements a basic linear SVM from scratch using the SMO algorithm,\n",
    "and compares its performance with scikit-learn's SVM classifier. \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
