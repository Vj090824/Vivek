{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020571c8-468e-47c3-872c-1e83634ed80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale the features\n",
    "    of a dataset within a specific range. It transforms the values of each feature in the dataset so that they\n",
    "    fall between a given minimum and maximum value, typically between 0 and 1. This scaling is achieved by\n",
    "    subtracting the minimum value of the feature and then dividing it by the range (maximum value - minimum value).\n",
    "\n",
    "The formula for Min-Max scaling of a feature \"x\" is as follows:\n",
    "\n",
    "Scaled_x = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "where \"min(x)\" is the minimum value of the feature \"x,\" and \"max(x)\" is the maximum value of the feature \"x.\"\n",
    "\n",
    "The purpose of Min-Max scaling is to bring all features to a similar scale, which can be particularly\n",
    "beneficial for machine learning algorithms that rely on distance calculations or \n",
    "gradient descent for optimization. It prevents features with large scales from dominating the learning \n",
    "process and ensures that all features contribute equally to the model's performance.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose we have a dataset with a single feature representing the age of people\n",
    "and their corresponding income as shown below:\n",
    "\n",
    "| Age (x) | Income (y) |\n",
    "|---------|------------|\n",
    "| 25      | $40,000    |\n",
    "| 30      | $45,000    |\n",
    "| 35      | $55,000    |\n",
    "| 40      | $60,000    |\n",
    "| 45      | $70,000    |\n",
    "\n",
    "To apply Min-Max scaling to the \"Age\" feature, we follow these steps:\n",
    "\n",
    "Step 1: Calculate the minimum and maximum values of the \"Age\" feature.\n",
    "- min(x) = 25\n",
    "- max(x) = 45\n",
    "\n",
    "Step 2: Apply the Min-Max scaling formula to each data point in the \"Age\" feature.\n",
    "\n",
    "Scaled Age (x) = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "Scaled Age (x) = (25 - 25) / (45 - 25) = 0 / 20 = 0.00\n",
    "Scaled Age (x) = (30 - 25) / (45 - 25) = 5 / 20 = 0.25\n",
    "Scaled Age (x) = (35 - 25) / (45 - 25) = 10 / 20 = 0.50\n",
    "Scaled Age (x) = (40 - 25) / (45 - 25) = 15 / 20 = 0.75\n",
    "Scaled Age (x) = (45 - 25) / (45 - 25) = 20 / 20 = 1.00\n",
    "\n",
    "The scaled \"Age\" feature now ranges between 0 and 1. We can use the same formula to apply \n",
    "Min-Max scaling to the \"Income\" feature or any other feature in the dataset\n",
    "to bring all the features to a common scale.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    The Unit Vector technique is a method used in feature scaling to normalize the data points\n",
    "    in a dataset. It rescales each feature (column) in the dataset so that they have a magnitude\n",
    "    of 1, effectively transforming them into unit vectors. The main idea is to preserve the direction\n",
    "    of the data points while ensuring they all fall on the surface of a unit hypersphere.\n",
    "\n",
    "To apply the Unit Vector technique to a dataset, for each feature (column) 'X', you calculate \n",
    "the unit vector 'X_unit' using the following formula:\n",
    "\n",
    "\\[X_{unit} = \\frac{X}{\\|X\\|}\\]\n",
    "\n",
    "where 'X' is the original feature values and '\\|X\\|' denotes the Euclidean norm (magnitude) of 'X'.\n",
    "\n",
    "The Min-Max scaling technique, on the other hand, is another feature scaling method that rescales the\n",
    "data to a fixed range, typically between 0 and 1. It transforms each data point 'X' in a feature to a\n",
    "new value 'X_scaled' using the following formula:\n",
    "\n",
    "\\[X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}\\]\n",
    "\n",
    "where 'X_min' and 'X_max' represent the minimum and maximum values of the feature 'X' in the dataset, respectively.\n",
    "\n",
    "Differences between Unit Vector scaling and Min-Max scaling:\n",
    "\n",
    "1. Magnitude vs. Range: Unit Vector scaling focuses on normalizing the magnitude of each feature to 1,\n",
    "while Min-Max scaling compresses the range of each feature into a fixed interval, usually [0, 1].\n",
    "\n",
    "2. Direction preservation: Unit Vector scaling preserves the direction of the data points,\n",
    "while Min-Max scaling doesn't necessarily maintain the direction.\n",
    "\n",
    "Example to illustrate Unit Vector scaling:\n",
    "\n",
    "Let's consider a dataset with two features, 'Age' and 'Income', represented by the following matrix:\n",
    "\n",
    "\n",
    "Original Data:\n",
    "Age  | Income\n",
    "--------------\n",
    " 30  |  50000\n",
    " 40  |  60000\n",
    " 25  |  55000\n",
    "```\n",
    "\n",
    "Step 1: Calculate the Euclidean norm (magnitude) of each data point:\n",
    "\n",
    "Magnitude = sqrt({Age}^2 + {Income}^2)\n",
    "\n",
    "\n",
    "Magnitude = [sqrt(30^2 + 50000^2), sqrt(40^2 + 60000^2), sqrt(25^2 + 55000^2)]\n",
    "Magnitude = [50000.25, 70710.68, 55005.42]\n",
    "\n",
    "\n",
    "Step 2: Calculate the Unit Vector for each data point:\n",
    "\n",
    "\n",
    "Age_unit = Age / Magnitude\n",
    "Income_unit = Income / Magnitude\n",
    "\n",
    "Unit Vector:\n",
    "Age_unit    | Income_unit\n",
    "--------------------------\n",
    "0.000599996 | 0.999999986\n",
    "0.000564189 | 0.999999988\n",
    "0.000454531 | 0.999999891\n",
    "\n",
    "\n",
    "Now, each feature has been scaled to a unit vector, preserving the direction of the original data points.\n",
    "The magnitude of each unit vector is 1.\n",
    "\n",
    "Comparing this to Min-Max scaling:\n",
    "\n",
    "If we apply Min-Max scaling to the 'Age' and 'Income' features, assuming 'Age' ranges\n",
    "from 20 to 50, and 'Income' ranges from 50000 to 60000:\n",
    "\n",
    "\n",
    "Age_scaled = (Age - 20) / (50 - 20)\n",
    "Income_scaled = (Income - 50000) / (60000 - 50000)\n",
    "\n",
    "Min-Max Scaled Data:\n",
    "Age_scaled  | Income_scaled\n",
    "---------------------------\n",
    "0.33333333  | 0.5\n",
    "0.66666667  | 1.0\n",
    "0.0         | 0.75\n",
    "\n",
    "\n",
    "In this case, Min-Max scaling compresses each feature into the [0, 1] range,\n",
    "but it does not preserve the original direction of the data points.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    PCA, which stands for Principal Component Analysis, is a popular statistical technique used\n",
    "    for dimensionality reduction and data visualization. It allows us to transform\n",
    "    a dataset with a high number of variables (or features) into a new, lower-dimensional space,\n",
    "    while still retaining the most important information present in the original data.\n",
    "    This reduction in dimensionality helps in simplifying data analysis and visualization, \n",
    "    making it easier to understand and interpret the underlying patterns.\n",
    "\n",
    "The fundamental idea behind PCA is to find a new set of orthogonal axes, called principal components,\n",
    "in the original feature space, where the first principal component explains the maximum variance in the data,\n",
    "the second component explains the second maximum variance, and so on.\n",
    "This allows us to capture the most important patterns in the data along these principal components.\n",
    "\n",
    "Here are the steps involved in performing PCA for dimensionality reduction:\n",
    "\n",
    "1. Standardize the data: If the features in the dataset have different scales,\n",
    "it is essential to standardize them (subtract mean and divide by standard deviation) \n",
    "so that all features have equal importance during PCA.\n",
    "\n",
    "2. Calculate the covariance matrix: Compute the covariance matrix based on the standardized data,\n",
    "which represents the relationships between different features.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: Solve the eigenvalue problem for the covariance matrix \n",
    "to find its eigenvectors and eigenvalues. The eigenvectors represent the directions of the principal\n",
    "components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the top k components: Sort the eigenvectors based on their corresponding eigenvalues\n",
    "in descending order and choose the top k components.\n",
    "These components will capture the most significant variance in the data.\n",
    "\n",
    "5. Project the data onto the new space: Use the selected k eigenvectors as\n",
    "the transformation matrix to project the original data onto the new lower-dimensional space.\n",
    "\n",
    "Example:\n",
    "Let's illustrate PCA with a simple example. Consider a dataset with two features, \"Height\" and \"Weight,\" \n",
    "and we want to reduce it to a single dimension using PCA.\n",
    "\n",
    "Original Dataset:\n",
    "\n",
    "| Height (inches) | Weight (lbs) |\n",
    "|-----------------|-------------|\n",
    "| 65              | 150         |\n",
    "| 70              | 160         |\n",
    "| 72              | 180         |\n",
    "| 63              | 135         |\n",
    "| 75              | 190         |\n",
    "\n",
    "\n",
    "Step 1: Standardize the data (we'll assume the mean and standard deviation for simplicity).\n",
    "\n",
    "| Height (standardized) | Weight (standardized) |\n",
    "|-----------------------|----------------------|\n",
    "| -1.11                 | -0.96                |\n",
    "| 0.56                  | -0.62                |\n",
    "| 1.11                  | 0.61                 |\n",
    "| -1.67                 | -1.30                |\n",
    "| 1.11                  | 1.26                 |\n",
    "\n",
    "\n",
    "Step 2: Calculate the covariance matrix (rounded for simplicity).\n",
    "\n",
    "| 0.97  0.88 |\n",
    "| 0.88  1.01 |\n",
    "\n",
    "\n",
    "Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "Eigenvectors:\n",
    "\n",
    "[ 0.71, -0.71 ]\n",
    "[ 0.71,  0.71 ]\n",
    "\n",
    "Eigenvalues:\n",
    "\n",
    "2.03\n",
    "0.95\n",
    "\n",
    "\n",
    "Step 4: Select the top principal component (the one with the highest eigenvalue).\n",
    "In this case, it's `[0.71, -0.71]`.\n",
    "\n",
    "Step 5: Project the data onto the new space.\n",
    "\n",
    "| New Dimension (PCA) |\n",
    "|---------------------|\n",
    "| -1.47               |\n",
    "| 0.29                |\n",
    "| 1.18                |\n",
    "| -2.17               |\n",
    "| 2.18                |\n",
    "\n",
    "\n",
    "Now, we have successfully reduced the dataset to a single dimension using PCA. \n",
    "This new single dimension retains most of the important information about the original data,\n",
    "and it is a linear combination of the \"Height\" and \"Weight\" features.\n",
    "The other dimensions (i.e., second principal component) were discarded as they contained less variance.\n",
    "This reduction is helpful when visualizing or analyzing the data,\n",
    "especially when dealing with high-dimensional datasets.\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "     \n",
    "                              \n",
    "                              \n",
    "                              \n",
    " \n",
    "                              \n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "                              \n",
    "                              \n",
    " \n",
    " Ans:     \n",
    "       Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used \n",
    "    in machine learning and data analysis. Its main objective is to transform high-dimensional\n",
    "    data into a lower-dimensional space while retaining as much of the original information as possible.\n",
    "The reduction in dimensionality achieved by PCA can be useful for various purposes, including feature extraction.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA can be utilized to \n",
    "extract the most important features or patterns from a dataset. These features are the principal components, \n",
    "which are linear combinations of the original features. The principal components are sorted in descending order \n",
    "based on their variance, where the first principal component explains the maximum variance in the data, \n",
    "the second one explains the second most significant variance, and so on.\n",
    "\n",
    "Here's a step-by-step example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Step 1: Collect the Data\n",
    "Let's consider a dataset with high-dimensional features. For this example, \n",
    "we'll use a simple dataset with two features: \"X\" and \"Y\".\n",
    "\n",
    "\n",
    "+----+----+\n",
    "|  X |  Y |\n",
    "+----+----+\n",
    "|  1 |  2 |\n",
    "|  2 |  3 |\n",
    "|  3 |  5 |\n",
    "|  4 |  6 |\n",
    "|  5 |  8 |\n",
    "+----+----+\n",
    "\n",
    "\n",
    "Step 2: Standardize the Data\n",
    "PCA is sensitive to the scale of the features,\n",
    "so it's important to standardize the data to have zero mean and unit variance.\n",
    "\n",
    "Step 3: Compute the Covariance Matrix\n",
    "The next step is to compute the covariance matrix from the standardized data.\n",
    "The covariance matrix will represent the relationships between the features.\n",
    "\n",
    "Step 4: Calculate the Eigenvectors and Eigenvalues\n",
    "The eigenvectors and eigenvalues of the covariance matrix are computed.\n",
    "The eigenvectors represent the directions of the principal components, \n",
    "and the corresponding eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "Step 5: Select Principal Components\n",
    "The principal components are ranked based on their eigenvalues, and a certain number of \n",
    "principal components are selected. These components constitute the new feature space.\n",
    "\n",
    "Step 6: Project Data onto the New Feature Space\n",
    "The original data is projected onto the new feature space formed by the selected principal components.\n",
    "\n",
    "Step 7: Feature Extraction\n",
    "At this point, the projected data represents the extracted features. The new feature space may have\n",
    "fewer dimensions than the original dataset, and each feature in this space is\n",
    "a linear combination of the original features.\n",
    "\n",
    "For example, if we choose to retain only one principal component from the above dataset,\n",
    "the new feature space will be one-dimensional, and the extracted feature will be the projection of\n",
    "the data onto that component. If we choose more principal components, the new feature space will have\n",
    "more dimensions, capturing more information from the original dataset.\n",
    "\n",
    "In summary, PCA can be used for feature extraction by transforming the original features into a new\n",
    "feature space composed of principal components that represent the most important patterns and \n",
    "variations in the data.                       \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "Ans:\n",
    "                              \n",
    "                              \n",
    " Min-Max scaling, also known as normalization, is a common data preprocessing technique used\n",
    "to scale numerical features to a specific range, typically between 0 and 1. This ensures that\n",
    "all features contribute equally to the analysis, regardless of their original scale.\n",
    "    When building a recommendation system for a food delivery service, using Min-Max scaling\n",
    "can be helpful to bring consistency and improve the performance of the model. \n",
    "Here's how Min-Max scaling can be applied to preprocess the data:\n",
    "\n",
    "Step 1: Understand the Data\n",
    "First, you need to examine the dataset and identify the numerical features that require scaling. \n",
    "In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "Step 2: Calculate the Min and Max Values\n",
    "Compute the minimum and maximum values for each of the numerical features in the dataset.\n",
    "This information will be used later in the scaling process.\n",
    "\n",
    "Step 3: Apply Min-Max Scaling Formula\n",
    "The Min-Max scaling formula is given by:\n",
    "\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "\n",
    "Where:\n",
    "- `X` is the original value of the feature.\n",
    "- `X_min` is the minimum value of the feature in the dataset.\n",
    "- `X_max` is the maximum value of the feature in the dataset.\n",
    "- `X_scaled` is the scaled value of the feature after applying Min-Max scaling.\n",
    "\n",
    "Step 4: Scale the Data\n",
    "Now, apply the Min-Max scaling formula to each numerical feature in the dataset.\n",
    "This will transform the features into a common range (usually 0 to 1).\n",
    "\n",
    "Step 5: Use Scaled Data for the Recommendation System\n",
    "Once the data is scaled, you can use it as input for building the recommendation system. \n",
    "The scaled features will ensure that no single feature dominates the recommendation process \n",
    "due to its original magnitude, providing a fair contribution to the overall recommendation algorithm.\n",
    "\n",
    "For example, let's say you have the following dataset for three food items:\n",
    "\n",
    "\n",
    "Item   Price  Rating  Delivery Time\n",
    "A      $15    4.5     30 minutes\n",
    "B      $10    4.0     25 minutes\n",
    "C      $25    4.8     40 minutes\n",
    "\n",
    "\n",
    "Step 1: Identify numerical features: Price and Rating are numerical, while Delivery Time is \n",
    "categorical and requires further preprocessing (e.g., label encoding).\n",
    "\n",
    "Step 2: Calculate Min and Max Values:\n",
    "\n",
    "Min Price: $10\n",
    "Max Price: $25\n",
    "Min Rating: 4.0\n",
    "Max Rating: 4.8\n",
    "\n",
    "\n",
    "Step 3: Apply Min-Max Scaling:\n",
    "\n",
    "Scaled Price (A):  (15 - 10) / (25 - 10) = 0.625\n",
    "Scaled Rating (A): (4.5 - 4.0) / (4.8 - 4.0) = 0.75\n",
    "\n",
    "Scaled Price (B):  (10 - 10) / (25 - 10) = 0.0\n",
    "Scaled Rating (B): (4.0 - 4.0) / (4.8 - 4.0) = 0.0\n",
    "\n",
    "Scaled Price (C):  (25 - 10) / (25 - 10) = 1.0\n",
    "Scaled Rating (C): (4.8 - 4.0) / (4.8 - 4.0) = 1.0\n",
    "\n",
    "\n",
    "Now, the Price and Rating features are scaled between 0 and 1, making them suitable for\n",
    "    the recommendation system, which can then use these scaled features to make fair\n",
    "and effective food recommendations based on various user preferences.               \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "    \n",
    "   \n",
    "                              \n",
    " Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "                              \n",
    "                                                         \n",
    "                              \n",
    "Ans:\n",
    "                              \n",
    "    Principal Component Analysis (PCA) is a popular technique used for\n",
    "dimensionality reduction in datasets with many features. \n",
    "When working on a project to predict stock prices, PCA can be employed to reduce the dimensionality \n",
    "of the dataset while preserving most of the relevant information. Reducing dimensionality is \n",
    "particularly important in this case as it can help to avoid the curse of dimensionality,\n",
    "improve model performance, and make the data more manageable.\n",
    "\n",
    "Here's how PCA can be used to reduce the dimensionality of the stock price prediction dataset:\n",
    "\n",
    "1. Data Preparation:\n",
    "                              Gather the dataset that contains various features, including company financial data \n",
    "(e.g., revenue, profit, debt-to-equity ratio) and market trends (e.g., interest rates, market indices,\n",
    "inflation rates). Ensure that the dataset is properly preprocessed, \n",
    "and features are appropriately scaled or normalized.\n",
    "\n",
    "2. Covariance Matrix Calculation: \n",
    "Compute the covariance matrix of the dataset. The covariance matrix represents the relationships\n",
    "between different features and shows how they vary together.\n",
    "\n",
    "3. Eigenvalue-Eigenvector Decomposition: \n",
    "Perform eigenvalue-eigenvector decomposition on the covariance matrix. \n",
    "This step helps in finding the principal components of the data. Principal components are new orthogonal \n",
    "(uncorrelated) variables that are linear combinations of the original features.\n",
    "They capture the most significant variation in the data.\n",
    "\n",
    "4. Ranking Principal Components: \n",
    "Sort the eigenvalues in descending order. The eigenvalues represent the amount of variance\n",
    "explained by each principal component. Higher eigenvalues indicate that the corresponding\n",
    "principal component carries more information about the data.\n",
    "\n",
    "5. Selecting the Number of Principal Components: \n",
    "        Decide on the number of principal components\n",
    "to retain in the reduced dataset. A common approach is to choose the number of principal \n",
    "components that capture a significant portion of the total variance.\n",
    "For example, you might set a threshold like retaining components that explain \n",
    "at least 95% or 99% of the total variance.\n",
    "\n",
    "6. Dimensionality Reduction:\n",
    "    Create a new dataset by selecting the top principal components based on \n",
    "the selected number from the previous step. This new dataset will have a reduced number\n",
    "of features compared to the original dataset.\n",
    "\n",
    "7. Model Building: Use the reduced dataset as input to build your stock price prediction model.\n",
    "Since the dimensionality has been reduced, the model can be trained more efficiently and\n",
    "might even have improved performance due to the removal of noise and irrelevant features.\n",
    "\n",
    "By applying PCA, you are transforming the original high-dimensional feature space into\n",
    "a lower-dimensional subspace that retains the most important information.\n",
    "This not only helps in mitigating the risk of overfitting but also speeds up the computation, \n",
    "    making the modeling process more manageable and interpretable.\n",
    "However, keep in mind that while PCA is a powerful technique.\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "  Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "Ans:\n",
    "                              \n",
    "    Min-Max scaling is a data normalization technique used to scale the values \n",
    "of a dataset to a specific range, typically between 0 and 1. In this case, \n",
    "you want to scale the values to a range of -1 to 1. The formula for Min-Max scaling is:\n",
    "\n",
    "Scaled_value = (value - min_value) / (max_value - min_value) * (max_range - min_range) + min_range\n",
    "\n",
    "Where:\n",
    "- \"value\" is the original value in the dataset.\n",
    "- \"min_value\" is the minimum value in the dataset.\n",
    "- \"max_value\" is the maximum value in the dataset.\n",
    "- \"min_range\" is the minimum value of the desired range (-1 in this case).\n",
    "- \"max_range\" is the maximum value of the desired range (1 in this case).\n",
    "\n",
    "Let's perform Min-Max scaling for the given dataset [1, 5, 10, 15, 20]:\n",
    "\n",
    "Step 1: Find the minimum and maximum values in the dataset.\n",
    "- min_value = 1\n",
    "- max_value = 20\n",
    "\n",
    "Step 2: Scale the values using the formula:\n",
    "\n",
    "Scaled_value = (value - min_value) / (max_value - min_value) * (max_range - min_range) + min_range\n",
    "\n",
    "For each value in the dataset:\n",
    "- For value 1:\n",
    "  Scaled_value = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0 * 2 - 1 = -1\n",
    "\n",
    "- For value 5:\n",
    "  Scaled_value = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 4 / 19 * 2 - 1 ≈ -0.0526\n",
    "\n",
    "- For value 10:\n",
    "  Scaled_value = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 9 / 19 * 2 - 1 ≈ 0.2632\n",
    "\n",
    "- For value 15:\n",
    "  Scaled_value = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 14 / 19 * 2 - 1 ≈ 0.5789\n",
    "\n",
    "- For value 20:\n",
    "  Scaled_value = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 19 / 19 * 2 - 1 = 1\n",
    "\n",
    "The Min-Max scaled dataset is approximately: [-1, -0.0526, 0.2632, 0.5789, 1].                          \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "   Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "                              \n",
    "                              \n",
    "                              \n",
    " Ans:\n",
    "                              \n",
    "                              \n",
    " Principal Component Analysis (PCA) is a popular technique for feature extraction and\n",
    "    dimensionality reduction. It is used to transform the original dataset into a new set \n",
    "    of uncorrelated variables called principal components.\n",
    "    These principal components are ordered in such a way that the first principal \n",
    "component explains the maximum variance in the data, the second principal\n",
    "component explains the second-highest variance, and so on.\n",
    "\n",
    "To determine the number of principal components to retain, one common approach is\n",
    "to analyze the explained variance ratio. The explained variance ratio is the proportion \n",
    "    of variance explained by each principal component. Retaining a sufficient number of\n",
    "principal components ensures that most of the variance in the data is preserved while reducing the dimensionality.\n",
    "\n",
    "Here are the steps to perform feature extraction using PCA and decide how many principal components to retain:\n",
    "\n",
    "Step 1: Standardize the data\n",
    "Before applying PCA, it's essential to standardize the data to have a mean of 0 and a standard\n",
    "    deviation of 1 for each feature. This step ensures that all features are on the same scale\n",
    "and have equal importance during the PCA process.\n",
    "\n",
    "Step 2: Compute the covariance matrix\n",
    "Calculate the covariance matrix of the standardized data. \n",
    "The covariance matrix describes the relationships between different features.\n",
    "\n",
    "Step 3: Calculate eigenvectors and eigenvalues\n",
    "The eigenvectors and eigenvalues of the covariance matrix represent the principal components\n",
    "        and their corresponding variances, respectively.\n",
    "\n",
    "Step 4: Sort eigenvalues and select principal components\n",
    "Sort the eigenvalues in descending order and choose the top 'k' principal components that explain \n",
    "a significant portion of the total variance. A common heuristic is to choose the number of \n",
    "principal components that explain, for example, 95% or 99% of the total variance.\n",
    "\n",
    "Step 5: Transform the data\n",
    "Transform the original data into the new lower-dimensional space spanned by the selected principal components.\n",
    "\n",
    "Regarding the specific dataset with features [height, weight, age, gender, blood pressure]\n",
    "the number of principal components to retain would depend on the variance explained by each component.\n",
    "Without having access to the actual data, it's not possible to provide an exact answer.\n",
    "However, I can provide some general guidance:\n",
    "\n",
    "1. The first few principal components usually explain most of the variance in the data.\n",
    "Retaining these components will preserve the essential information while reducing the dimensionality significantly.\n",
    "\n",
    "2. The number of principal components you choose to retain may vary based on the specific application\n",
    "and the level of dimensionality reduction required.\n",
    "\n",
    "3. As a starting point, you could aim to retain enough principal components to explain,\n",
    "for example, 95% or 99% of the total variance in the data.\n",
    "\n",
    "Remember that the trade-off in PCA is between reducing dimensionality and preserving enough \n",
    "information to perform well on the task at hand. If you decide to retain only a subset of the\n",
    "    principal components, it's essential to evaluate the performance of your chosen model \n",
    "(e.g., regression, classification) on the reduced dataset to ensure it still performs \n",
    "well for your specific use case.                             \n",
    "                              \n",
    "                              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
