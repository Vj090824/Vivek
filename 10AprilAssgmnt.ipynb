{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c6974-c551-414e-8ee4-5154949e7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    To find the probability that an employee is a smoker given that they use the health insurance plan,\n",
    "    you can use conditional probability. You can use the formula for conditional probability:\n",
    "\n",
    "P(Smoker | Uses Health Insurance Plan) = P(Smoker and Uses Health Insurance Plan)\n",
    "/ P(Uses Health Insurance Plan)\n",
    "\n",
    "In this case, you know that 40% of the employees who use the health insurance\n",
    "plan are smokers, so:\n",
    "\n",
    "P(Smoker and Uses Health Insurance Plan) = 40% = 0.40\n",
    "\n",
    "To find P(Uses Health Insurance Plan), you can use the fact that 70% of the employees \n",
    "use the company's health insurance plan:\n",
    "\n",
    "P(Uses Health Insurance Plan) = 70% = 0.70\n",
    "\n",
    "Now, you can plug these values into the formula:\n",
    "\n",
    "P(Smoker | Uses Health Insurance Plan) = 0.40 / 0.70 â‰ˆ 0.5714\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use\n",
    "the health insurance plan is approximately 0.5714 or 57.14%.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    \n",
    "    Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of \n",
    "    the Naive Bayes classifier, which is a probabilistic machine learning algorithm commonly \n",
    "    used for text classification and other categorization tasks. The main difference between\n",
    "    them lies in the type of data they are designed to work with and the underlying assumptions they make:\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - **Input Data Type**: Bernoulli Naive Bayes is typically used for binary data, where\n",
    "each feature can take one of two values, usually 0 or 1. It's well-suited for text data\n",
    "where you want to represent the presence or absence of specific words in a document.\n",
    "   - **Assumption**: It assumes that the features are binary or can be converted into binary values.\n",
    "   - **Example**: Spam email classification, sentiment analysis\n",
    "(where words are either present or absent in a text).\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - **Input Data Type**: Multinomial Naive Bayes is designed for categorical\n",
    "data where features represent discrete counts. It's commonly used for text data\n",
    "where the features represent word counts or term frequencies.\n",
    "   - **Assumption**: It assumes that the features are multinomially distributed,\n",
    "    which means they represent counts of events (e.g., word occurrences).\n",
    "   - **Example**: Document classification, topic modeling, and text categorization \n",
    "tasks where you want to use word frequencies or counts as features.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes\n",
    "depends on the nature of your data. If you're dealing with binary features\n",
    "(e.g., presence or absence of words), Bernoulli Naive Bayes may be more appropriate.\n",
    "If you're working with categorical data representing counts or frequencies \n",
    "(e.g., word counts), then Multinomial Naive Bayes is a better choice. It's \n",
    "important to select the variant that aligns with the specific characteristics\n",
    "of your dataset to achieve better classification results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "Bernoulli Naive Bayes is a variant of the Naive Bayes classifier that is specifically designed for\n",
    "binary or boolean data, where features are represented as either 0 or 1. This classifier is commonly\n",
    "used in text classification tasks, such as spam detection, sentiment analysis, and document categorization.\n",
    "\n",
    "Handling missing values in Bernoulli Naive Bayes can be a bit different from handling missing values\n",
    "in other types of machine learning algorithms because of its binary nature.\n",
    "Here's how Bernoulli Naive Bayes typically deals with missing values:\n",
    "\n",
    "1. **Missing Values as a Third Category:** In Bernoulli Naive Bayes, missing values\n",
    "(i.e., features that are neither 0 nor 1) are often treated as a third category or state.\n",
    "This means that when you have missing data for a particular feature in a given observation,\n",
    "you don't simply ignore it or impute it with a specific value. Instead,\n",
    "you explicitly consider the missing value as a separate category when calculating probabilities.\n",
    "\n",
    "2. **Probability Estimation:** When calculating the conditional probabilities\n",
    "for each feature given the class label (e.g., P(feature=1 | class)), Bernoulli\n",
    "Naive Bayes takes into account not only the instances where the feature is explicitly\n",
    "1 but also the instances where the feature is missing. This means that the presence \n",
    "of missing values is factored into the probability calculations.\n",
    "\n",
    "3. **Handling Missing Data During Prediction:** When you make predictions using a\n",
    "Bernoulli Naive Bayes classifier on new data that contains missing values, the classifier\n",
    "will incorporate the missing values into its calculations and provide probabilities\n",
    "for each class accordingly.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes handles missing values by treating them as a \n",
    "distinct category when estimating probabilities for binary features. This approach allows\n",
    "the classifier to make predictions on instances with missing values without the need for\n",
    "imputation or data preprocessing techniques commonly used in other algorithms. However, \n",
    "it's essential to be cautious when using Bernoulli Naive Bayes with missing data, \n",
    "as the assumptions of the Naive Bayes model may not always hold true, especially \n",
    "when dealing with missing data patterns that are not random or missing completely at random.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    \n",
    "    Yes, Gaussian Naive Bayes (GNB) can be used for multi-class classification tasks. It's\n",
    "    a variation of the Naive Bayes algorithm that assumes that the features follow a\n",
    "    Gaussian (normal) distribution. While the \"Naive\" part of Naive Bayes assumes\n",
    "    independence between features, the Gaussian version specifically models the \n",
    "    likelihood of each feature as a Gaussian distribution.\n",
    "\n",
    "When applied to multi-class classification, GNB can assign a class label to an input\n",
    "based on the Gaussian distribution of features for each class. It calculates the\n",
    "probability that a data point belongs to each class and assigns it to the class with the highest probability.\n",
    "\n",
    "In summary, Gaussian Naive Bayes is a suitable algorithm for multi-class \n",
    "classification problems when the assumption of Gaussian-distributed features holds\n",
    "reasonably well for the data. However, it's important to note that this assumption\n",
    "may not always hold in practice, and the performance of GNB can be sensitive to\n",
    "violations of this assumption. In such cases, you may want to consider other classification\n",
    "algorithms that do not make strong distributional assumptions, such as decision trees,\n",
    "random forests, or support vector machines.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    To complete the assignment, you will need to follow these steps:\n",
    "\n",
    "**Data Preparation:**\n",
    "\n",
    "1. Download the \"Spambase Data Set\" from the UCI Machine Learning Repository:\n",
    "    [Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/Spambase).\n",
    "\n",
    "2. Load the dataset into your Python environment using libraries like pandas.\n",
    "\n",
    "3. Split the data into features (X) and the target variable (y), where X contains\n",
    "the input features and y contains the labels (spam or not spam).\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "4. Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes\n",
    "classifiers using scikit-learn. You can use the following classes from scikit-learn:\n",
    "   - `BernoulliNB` for Bernoulli Naive Bayes.\n",
    "   - `MultinomialNB` for Multinomial Naive Bayes.\n",
    "   - `GaussianNB` for Gaussian Naive Bayes.\n",
    "\n",
    "5. Perform 10-fold cross-validation for each classifier to evaluate their performance.\n",
    "You can use `cross_val_score` from scikit-learn for this purpose.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "6. Report the following performance metrics for each classifier:\n",
    "   - Accuracy: The proportion of correctly classified instances.\n",
    "   - Precision: The ratio of true positive predictions to the total positive predictions.\n",
    "   - Recall: The ratio of true positive predictions to the total actual positive instances.\n",
    "   - F1 score: The harmonic mean of precision and recall.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "7. Discuss the results you obtained, including:\n",
    "   - Which variant of Naive Bayes performed the best in terms of the reported metrics?\n",
    "   - Why do you think that is the case?\n",
    "   - Any observed limitations of Naive Bayes in the context of this dataset.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "8. Summarize your findings, highlighting which Naive Bayes variant performed the best and why.\n",
    "Suggest possible reasons for the observed results.\n",
    "\n",
    "9. Provide some suggestions for future work, such as exploring different feature engineering \n",
    "techniques or considering other machine learning algorithms to improve spam classification performance.\n",
    "\n",
    "Here's a Python code template to help you get started:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Initialize Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and report metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "for clf, name in [(bernoulli_nb, 'Bernoulli Naive Bayes'),\n",
    "(multinomial_nb, 'Multinomial Naive Bayes'), (gaussian_nb, 'Gaussian Naive Bayes')]:\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring=metrics)\n",
    "    print(f\"{name}:\")\n",
    "    for metric, score in zip(metrics, scores):\n",
    "        print(f\"{metric}: {score:.2f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "Remember to replace `\"spambase.data\"` with the actual file path of the dataset you downloaded.\n",
    "This code will provide you with the performance metrics for each Naive Bayes variant\n",
    "and help you discuss and conclude your findings. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
